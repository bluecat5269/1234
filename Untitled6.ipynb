{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#dlib讀取\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import dlib\n",
    "import numpy as np\n",
    "import shutil\n",
    "from PIL import Image\n",
    "#神經網路用\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D \n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "\n",
    "def go():\n",
    "    \n",
    "    #圖片爬蟲\n",
    "    dataurl = 'https://www.google.com.tw/search?ei=MswPXJnSDMaT8wWwoozQDA&yv=3&q={}&tbm=isch&vet=10ahUKEwiZs6TsgJjfAhXGybwKHTARA8oQuT0IMigB.MswPXJnSDMaT8wWwoozQDA.i&ved=0ahUKEwiZs6TsgJjfAhXGybwKHTARA8oQuT0IMigB&ijn=1&start={}&asearch=ichunk&async=_id:rg_s,_pms:s,_fmt:pc'\n",
    "\n",
    "    def getIdolImg(keyword, dstpath):\n",
    "        for i in range(20):#抓幾頁資料\n",
    "            res = requests.get(dataurl.format(keyword, i * 100))\n",
    "            soup = BeautifulSoup(res.text, 'lxml')\n",
    "            for ele in soup.select('img'):\n",
    "                imgurl = ele.get('src')\n",
    "                if imgurl:\n",
    "                    fname  = imgurl.split('tbn:')[1]\n",
    "                    with open(dstpath + fname + '.jpg', 'wb') as f:\n",
    "                        res2 = requests.get(imgurl)\n",
    "                        f.write(res2.content)\n",
    "                else:\n",
    "                    imgurl = ele.get('data-src')\n",
    "                    fname = imgurl.split('tbn:')[1]\n",
    "        print(str(keyword)+\"資料下載完成\")\n",
    "              \n",
    "    if not os.path.exists('masaminagasawa/'):\n",
    "        os.mkdir('masaminagasawa/')\n",
    "    getIdolImg('長澤まさみ', 'masaminagasawa/')\n",
    "\n",
    "    if not os.path.exists('aragakiyui'):\n",
    "        os.mkdir('aragakiyui/')\n",
    "    getIdolImg('新垣結衣', 'aragakiyui/')\n",
    "\n",
    "    if not os.path.exists('satomiishihara'):\n",
    "        os.mkdir('satomiishihara/')\n",
    "    getIdolImg('石原さとみ', 'satomiishihara/')\n",
    "\n",
    "    #擷取臉部\n",
    "    def extractFace(srcpath, dstpath):\n",
    "        if not os.path.exists(srcpath):\n",
    "            os.mkdir(srcpath)\n",
    "        if not os.path.exists(dstpath):\n",
    "            os.mkdir(dstpath)\n",
    "        detector = dlib.get_frontal_face_detector()\n",
    "        #predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "        for fname in os.listdir(srcpath):\n",
    "            img = Image.open(srcpath + fname)\n",
    "            imgary = cv.imread(srcpath + fname)\n",
    "            img_gray = cv.cvtColor(imgary, cv.COLOR_RGB2GRAY)\n",
    "            faces =detector(img_gray, 0)\n",
    "            if len(faces) == 1:\n",
    "                for k, d in enumerate(faces):\n",
    "                    x=d.left()\n",
    "                    y=d.top()\n",
    "                    w=d.right()-d.left()\n",
    "                    h=d.bottom()-d.top()\n",
    "                #x,y,w,h = faces[0]\n",
    "                crpim = img.crop((x,y, x + w, y + h)).resize((128,128))\n",
    "                crpim.save(dstpath + fname)\n",
    "        print(str(srcpath)+\"臉部擷取完成\")\n",
    "            \n",
    "    srcpath = 'masaminagasawa/' \n",
    "    dstpath = 'masaminagasawaFace/'\n",
    "    extractFace(srcpath, dstpath)\n",
    "\n",
    "    srcpath = 'aragakiyui/' \n",
    "    dstpath = 'aragakiyuiFace/'\n",
    "    extractFace(srcpath, dstpath)\n",
    "\n",
    "    srcpath = 'satomiishihara/' \n",
    "    dstpath = 'satomiishiharaFace/'\n",
    "    extractFace(srcpath, dstpath)\n",
    "    \n",
    "    #放圖片進入test和train(3:7)\n",
    "    def Classify(filename):\n",
    "        #建立test+train資料夾\n",
    "        if not os.path.exists('testing'):\n",
    "            os.mkdir('testing/')\n",
    "        if not os.path.exists('training'):\n",
    "            os.mkdir('training/')\n",
    "            \n",
    "        '''if not os.path.exists('training'+filename):\n",
    "            a='training/'+filename+'/'\n",
    "            os.mkdir(a)'''\n",
    "\t\t\t\n",
    "        #將圖片放入\n",
    "        a='testing/'+filename+'/'\n",
    "        shutil.copytree(filename,a)\n",
    "        \n",
    "        a='training/'+filename+'/'\n",
    "        shutil.copytree(filename,a)\n",
    "        \n",
    "        #圖片數量比例(test,train)(3:7)\n",
    "        #train\n",
    "        f=[]\n",
    "        for fname in os.listdir('training/'+filename+'/'):\n",
    "            a=os.path.join('training/'+filename+'/',fname)\n",
    "            f.append(a)\n",
    "        num=len(f)\n",
    "        \n",
    "        trainnum=int(num*0.7)\n",
    "        \n",
    "        for i in range(trainnum,num):\n",
    "            os.remove(f[i])\n",
    "        \n",
    "        #test\n",
    "        f1=[]\n",
    "        for fname in os.listdir('testing/'+filename+'/'):\n",
    "            a=os.path.join('testing/'+filename+'/',fname)\n",
    "            f1.append(a)\n",
    "        num=len(f1)\n",
    "        \n",
    "        #testnum=int(num*0.3)\n",
    "        \n",
    "        for i in range(0,trainnum):\n",
    "            os.remove(f1[i])\n",
    "        \n",
    "        print(filename+'圖片放入完成')\n",
    "        \n",
    "    Classify('masaminagasawaFace')\n",
    "    Classify('aragakiyuiFace')\n",
    "    Classify('satomiishiharaFace')\n",
    "    \n",
    "    #數據集增強(增加資料)\n",
    "    def Datasetenhancement(filename):\n",
    "        def contrast_img(img1, c, b):  # 亮度就是每个像素所有通道都加上b\n",
    "            rows, cols, channels = img1.shape\n",
    "\n",
    "            # 新建全零(黑色)图片数组:np.zeros(img1.shape, dtype=uint8)\n",
    "            blank = np.zeros([rows, cols, channels], img1.dtype)\n",
    "            dst = cv.addWeighted(img1, c, blank, 1-c, b)\n",
    "            #cv.imshow('original_img', img)\n",
    "            #cv.imshow(\"contrast_img\", dst)\n",
    "            return dst\n",
    "\n",
    "        f=[]\n",
    "        for fname in os.listdir('training/'+filename+'/'):\n",
    "            a=os.path.join('training/'+filename+'/',fname)\n",
    "            f.append(a)\n",
    "        a=0\n",
    "        b=10\n",
    "        for i in f:\n",
    "            img = cv.imread(i, cv.IMREAD_COLOR)\n",
    "            bright=contrast_img(img, 1.5, 3)#亮\n",
    "            back=contrast_img(img, 0.8, 3)#暗\n",
    "            i=i.replace('.jpg',str(a)+'.jpg',1)\n",
    "            cv.imwrite( i, bright)\n",
    "            i=i.replace('.jpg',str(b)+'.jpg',1)\n",
    "            cv.imwrite( i, back)\n",
    "            a+=1\n",
    "            b+=1\n",
    "            \n",
    "        print(filename+'圖片處理完成')\n",
    "        \n",
    "    Datasetenhancement('masaminagasawaFace')\n",
    "    Datasetenhancement('aragakiyuiFace')\n",
    "    Datasetenhancement('satomiishiharaFace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#dlib讀取\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import dlib\n",
    "import numpy as np\n",
    "import shutil\n",
    "from PIL import Image\n",
    "#神經網路用\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D \n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go2():\n",
    "    #建立Convolution Neural Network 模型\n",
    "\n",
    "    # Initialising the CNN\n",
    "    classifier = Sequential()\n",
    "\n",
    "    # Convolution\n",
    "    classifier.add(Conv2D(32, (3, 3), input_shape = (128,128,3), activation = 'relu'))\n",
    "\n",
    "    # Max Pooling\n",
    "    classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "    # Convolution\n",
    "    classifier.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "\n",
    "    # Max Pooling\n",
    "    classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "\n",
    "    # Flattening\n",
    "    classifier.add(Flatten())\n",
    "\n",
    "    # Fully Connected\n",
    "    classifier.add(Dense(units = 128, activation = 'relu')) \n",
    "    classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "    classifier.add(Dense(units = 3, activation = 'softmax'))\n",
    "\n",
    "    classifier.summary()\n",
    "    \n",
    "    classifier.compile(optimizer = 'adam', \n",
    "                        loss ='categorical_crossentropy', \n",
    "                     metrics = ['accuracy'])\n",
    "\n",
    "    #讀入訓練與測試資料集\n",
    "    #藉由修改資料，增加資料量\n",
    "    train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,     \n",
    "                                   zoom_range = 0.2,\n",
    "                                  # rotation_range=0.5,\n",
    "                                   horizontal_flip = True,\n",
    "                                   featurewise_center = False,             #是否使輸入資料去中心化（均值為0），\n",
    "                                   samplewise_center  = False,             #是否使輸入資料的每個樣本均值為0\n",
    "                                   featurewise_std_normalization = False,  #是否資料標準化（輸入資料除以資料集的標準差）\n",
    "                                   samplewise_std_normalization  = False,  #是否將每個樣本資料除以自身的標準差\n",
    "                                   zca_whitening = False,                  #是否對輸入資料施以ZCA白化\n",
    "                                   rotation_range = 20,                    #資料提升時圖片隨機轉動的角度(範圍為0～180)\n",
    "                                   width_shift_range  = 0.2,               #資料提升時圖片水平偏移的幅度（單位為圖片寬度的佔比，0~1之間的浮點數）\n",
    "                                   height_shift_range = 0.2,               #同上，只不過這裡是垂直\n",
    "                                   #horizontal_flip = True,                 #是否進行隨機水平翻轉\n",
    "                                   vertical_flip = False\n",
    "                                  )\n",
    "\n",
    "    test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "    training_set = train_datagen.flow_from_directory(\n",
    "        'training/', target_size = (128, 128),\n",
    "        batch_size = 10,\n",
    "        class_mode = 'categorical')\n",
    "\n",
    "    test_set = test_datagen.flow_from_directory(\n",
    "        'testing/', target_size = (128, 128),\n",
    "        batch_size = 10, \n",
    "        class_mode = 'categorical')\n",
    "\n",
    "    #建立模型\n",
    "    history = classifier.fit_generator(training_set,\n",
    "        nb_epoch=10,\n",
    "        nb_val_samples=10,\n",
    "        steps_per_epoch = 30,\n",
    "        verbose = 1,\n",
    "        validation_data = test_set)\n",
    "    \n",
    "    #辨識照片中出現的\n",
    "    im = Image.open('unnamed.jpg')\n",
    "    print(im)\n",
    "    \n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    #face_cascade = cv.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "    img = cv.imread('unnamed.jpg')\n",
    "    img_gray = cv.cvtColor(img, cv.COLOR_RGB2GRAY)\n",
    "    faces =detector(img_gray, 0)\n",
    "    print(faces)\n",
    "\n",
    "    transform_dic = {\n",
    "        'masaminagasawaFace'  : 'masaminagasawa',\n",
    "        'aragakiyuiFace'  : 'aragakiyui',\n",
    "        'satomiishiharaFace'  : 'satomiishihara'\n",
    "    }\n",
    "    name_dic = {v:transform_dic.get(k) for k,v in training_set.class_indices.items()}\n",
    "    print(name_dic)\n",
    "\n",
    "    font = cv.FONT_HERSHEY_PLAIN\n",
    "    \n",
    "    for i in range(len(faces)):\n",
    "    # enumerate方法同时返回数据对象的索引和数据，k为索引，d为faces中的对象\n",
    "        for k, d in enumerate(faces):\n",
    "            # 用红色矩形框出人脸\n",
    "            x=d.left()\n",
    "            y=d.top()\n",
    "            w=d.right()-d.left()\n",
    "            h=d.bottom()-d.top()\n",
    "            box = (d.left(), d.top(),d.right(), d.bottom())\n",
    "            crpim = im.crop(box).resize((128,128))\n",
    "            target_image = image.img_to_array(crpim)\n",
    "            target_image = np.expand_dims(target_image, axis = 0)\n",
    "            res = classifier.predict_classes(target_image)[0]\n",
    "            #cv.rectangle(img, (d.left(), d.top()), (d.right(), d.bottom()), (0, 0, 255))\n",
    "            cv.rectangle(img,(x,y),(x+w,y+h),(14,201,255),2)\n",
    "            cv.putText(img,name_dic.get(res), (x + int(w/3)-70, y-10), font, 1.5, (14,201,255), 3)\n",
    "\n",
    "    #%pylab inline\n",
    "    plt.figure(figsize=(30,20))\n",
    "    plt.imshow(cv.cvtColor(img,cv.COLOR_BGR2RGB))\n",
    "\n",
    "    cv.namedWindow('img', cv.WINDOW_NORMAL)  #正常視窗大小\n",
    "    cv.imshow('img', img)                     #秀出圖片\n",
    "    cv.imwrite( \"result.jpg\", img)           #保存圖片\n",
    "    cv.waitKey(0)                             #等待按下任一按鍵\n",
    "    cv.destroyAllWindows()                    #關閉視窗\n",
    "    \n",
    "    score=classifier.evaluate(test_set)\n",
    "\n",
    "    loss,acc=score\n",
    "\n",
    "    print('正確率',acc)\n",
    "\n",
    "    classifier.save('myCNN2.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 126, 126, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 63, 63, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 61, 61, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 57600)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               7372928   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 7,409,219\n",
      "Trainable params: 7,409,219\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Found 381 images belonging to 3 classes.\n",
      "Found 58 images belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\r r\\lib\\site-packages\\ipykernel_launcher.py:71: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "C:\\Users\\user\\Anaconda3\\envs\\r r\\lib\\site-packages\\ipykernel_launcher.py:71: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., steps_per_epoch=30, verbose=1, validation_data=<keras.pre..., epochs=10, validation_steps=10)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20/30 [===================>..........] - ETA: 2s - loss: 1.7091 - accuracy: 0.3979"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-0303aecc548c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#go()#測試成功\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgo2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#正確率50%~60%左右，還可調整神經元數，維度調整，ImageDataGenerator參數調整，或轉成使用YOLO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-bc48976c690b>\u001b[0m in \u001b[0;36mgo2\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         validation_data = test_set)\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;31m#辨識照片中出現的\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\r r\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\r r\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1732\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\r r\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m                                             reset_metrics=False)\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\r r\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1514\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\r r\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3740\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3742\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\r r\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m     \"\"\"\n\u001b[1;32m-> 1081\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\r r\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1121\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\r r\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\r r\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\r r\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#go()#測試成功\n",
    "go2()#正確率50%~60%左右，還可調整神經元數，維度調整，ImageDataGenerator參數調整，或轉成使用YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
